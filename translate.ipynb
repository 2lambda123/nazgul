{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sockeye\n",
    "import mosestokenizer\n",
    "import html\n",
    "\n",
    "import mxnet as mx\n",
    "import sentencepiece as spm\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from truecaser import applytc\n",
    "from time import time\n",
    "from sockeye.translate import inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File path constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOCKEYE_MODEL_FOLDER_ENETLV = ['en-et-lv-model']\n",
    "TRUECASE_MODEL_ENETLV = 'preprocessing-models/joint-truecase-enetlv.tc'\n",
    "SENTENCEPIECE_MODEL_ENETLV = 'preprocessing-models/sp.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:MosesTokenizer] executing argv ['perl', '/usr/local/lib/python3.6/dist-packages/mosestokenizer/tokenizer-v1.1.perl', '-q', '-l', 'en', '-b', '-a']\n",
      "[INFO:MosesTokenizer] spawned process 28076\n"
     ]
    }
   ],
   "source": [
    "my_tokenizer = mosestokenizer.MosesTokenizer('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:MosesDetokenizer] executing argv ['perl', '/usr/local/lib/python3.6/dist-packages/mosestokenizer/detokenizer.perl', '-q', '-b', '-l', 'en']\n",
      "[INFO:MosesDetokenizer] spawned process 28098\n"
     ]
    }
   ],
   "source": [
    "my_detokenizer = mosestokenizer.MosesDetokenizer('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_truecaser_enetlv = applytc.loadModel(TRUECASE_MODEL_ENETLV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_segmenter_enetlv = spm.SentencePieceProcessor()\n",
    "my_segmenter_enetlv.Load(SENTENCEPIECE_MODEL_ENETLV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translator(model_folders):\n",
    "    ctx = mx.gpu()\n",
    "    models, source_vocabs, target_vocab = inference.load_models(\n",
    "        context=ctx,\n",
    "        max_input_len=None,\n",
    "        beam_size=5,\n",
    "        batch_size=1,\n",
    "        model_folders=model_folders,\n",
    "        checkpoints=None,\n",
    "        softmax_temperature=None,\n",
    "        max_output_length_num_stds=2,\n",
    "        decoder_return_logit_inputs=False,\n",
    "        cache_output_layer_w_b=False)\n",
    "    return inference.Translator(context=ctx,\n",
    "                                ensemble_mode=\"linear\",\n",
    "                                bucket_source_width=10,\n",
    "                                length_penalty=inference.LengthPenalty(1.0, 0.0),\n",
    "                                beam_prune=0,\n",
    "                                beam_search_stop='all',\n",
    "                                models=models,\n",
    "                                source_vocabs=source_vocabs,\n",
    "                                target_vocab=target_vocab,\n",
    "                                restrict_lexicon=None,\n",
    "                                store_beam=False,\n",
    "                                strip_unknown_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:sockeye.inference] Loading 1 model(s) from ['en-et-lv-model'] ...\n",
      "[INFO:sockeye.vocab] Vocabulary (34089 words) loaded from \"en-et-lv-model/vocab.src.0.json\"\n",
      "[INFO:sockeye.vocab] Vocabulary (7 words) loaded from \"en-et-lv-model/vocab.src.1.json\"\n",
      "[INFO:sockeye.vocab] Vocabulary (8 words) loaded from \"en-et-lv-model/vocab.src.2.json\"\n",
      "[INFO:sockeye.vocab] Vocabulary (34089 words) loaded from \"en-et-lv-model/vocab.trg.0.json\"\n",
      "[INFO:sockeye.inference] Model version: 1.18.51\n",
      "[INFO:sockeye.model] ModelConfig loaded from \"en-et-lv-model/config\"\n",
      "[INFO:sockeye.model] Config[_frozen=True, config_data=Config[_frozen=True, data_statistics=Config[_frozen=True, average_len_target_per_bucket=[5.925335796339093, 13.455985766027595, 23.04356145777397, 32.409491679603356, 41.705934319477535, 51.04894822540166, 60.30271898744638, 69.60858214192915, 78.81654387987426, 87.80222051615205], buckets=[(10, 10), (20, 20), (30, 30), (40, 40), (50, 50), (60, 60), (70, 70), (80, 80), (90, 90), (100, 100)], length_ratio_mean=1.0480516107695435, length_ratio_std=0.37196082581613815, max_observed_len_source=100, max_observed_len_target=100, num_discarded=130549, num_sents=22756733, num_sents_per_bucket=[8048703, 6148670, 3391117, 2181026, 1317590, 757862, 435493, 249355, 142252, 84665], num_tokens_source=435155439, num_tokens_target=435160667, num_unks_source=0, num_unks_target=0, size_vocab_source=34089, size_vocab_target=34089], max_seq_len_source=100, max_seq_len_target=100, num_source_factors=3, source_with_eos=True], config_decoder=Config[_frozen=True, act_type=relu, attention_heads=8, conv_config=None, dropout_act=0.1, dropout_attention=0.1, dropout_prepost=0.1, dtype=float32, feed_forward_num_hidden=2048, lhuc=False, max_seq_len_source=100, max_seq_len_target=100, model_size=512, num_layers=6, positional_embedding_type=fixed, postprocess_sequence=dr, preprocess_sequence=n, use_lhuc=False], config_embed_source=Config[_frozen=True, dropout=0.0, dtype=float32, factor_configs=[Config[_frozen=False, num_embed=4, vocab_size=7], Config[_frozen=False, num_embed=4, vocab_size=8]], num_embed=512, num_factors=3, vocab_size=34089], config_embed_target=Config[_frozen=True, dropout=0.0, dtype=float32, factor_configs=None, num_embed=512, num_factors=1, vocab_size=34089], config_encoder=Config[_frozen=True, act_type=relu, attention_heads=8, conv_config=None, dropout_act=0.1, dropout_attention=0.1, dropout_prepost=0.1, dtype=float32, feed_forward_num_hidden=2048, lhuc=False, max_seq_len_source=100, max_seq_len_target=100, model_size=520, num_layers=6, positional_embedding_type=fixed, postprocess_sequence=dr, preprocess_sequence=n, use_lhuc=False], config_loss=Config[_frozen=True, label_smoothing=0.1, name=cross-entropy, normalization_type=valid, vocab_size=34089], lhuc=False, vocab_source_size=34089, vocab_target_size=34089, weight_normalization=False, weight_tying=False, weight_tying_type=None]\n",
      "[INFO:sockeye.encoder] sockeye.encoder.EncoderSequence dtype: float32\n",
      "[INFO:sockeye.encoder] sockeye.encoder.AddSinCosPositionalEmbeddings dtype: float32\n",
      "[INFO:sockeye.encoder] sockeye.encoder.TransformerEncoder dtype: float32\n",
      "[INFO:sockeye.decoder] sockeye.decoder.TransformerDecoder dtype: float32\n",
      "[INFO:sockeye.encoder] sockeye.encoder.AddSinCosPositionalEmbeddings dtype: float32\n",
      "[INFO:sockeye.encoder] sockeye.encoder.Embedding dtype: float32\n",
      "[INFO:sockeye.encoder] sockeye.encoder.Embedding dtype: float32\n",
      "[INFO:sockeye.model] Loaded params from \"en-et-lv-model/params.best\"\n",
      "[INFO:sockeye.inference] 1 model(s) loaded in 1.5189s\n",
      "[INFO:sockeye.inference] Translator (1 model(s) beam_size=5 beam_prune=off beam_search_stop=all ensemble_mode=None batch_size=1 buckets_source=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100] avoiding=0)\n"
     ]
    }
   ],
   "source": [
    "my_translator_enetlv = get_translator(SOCKEYE_MODEL_FOLDER_ENETLV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing, translation, postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence, lang_factor, style_factor,\n",
    "               tokenizer, truecaser, segmenter):\n",
    "    tokenized_sentence = html.unescape(' '.join(tokenizer(sentence)))\n",
    "    truecased_sentence = applytc.processLine(truecaser,\n",
    "                                             tokenized_sentence)\n",
    "    segmented_sentence = ' '.join([x\n",
    "                                   for x in segmenter.EncodeAsPieces(truecased_sentence)])\n",
    "    factored_sentence = ' '.join([x + '|' + lang_factor + '|' + style_factor\n",
    "                                  for x in segmented_sentence.split()])\n",
    "    \n",
    "    return factored_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(sentence, segmenter, detokenizer):\n",
    "    de_segmented_sentence = segmenter.DecodePieces(sentence.split())\n",
    "    de_truecased_sentence = de_segmented_sentence[0].upper() + de_segmented_sentence[1:]\n",
    "    de_tokenized_sentence = detokenizer(de_truecased_sentence.split())\n",
    "    \n",
    "    return de_tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(sentence, t):\n",
    "    trans_inputs = inference.make_input_from_factored_string(sentence_id=1, factored_string=sentence, translator=t)\n",
    "    outputs = t.translate([trans_inputs])\n",
    "    return outputs[0].translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, lang_factor, style_factor,\n",
    "              tokenizer, detokenizer, truecaser, segmenter,\n",
    "              translator):\n",
    "    translation = forward(preprocess(sentence, lang_factor, style_factor,\n",
    "                                     tokenizer, truecaser, segmenter),\n",
    "                          translator)\n",
    "    postprocessed_translation = postprocess(translation, segmenter, detokenizer)\n",
    "    \n",
    "    return postprocessed_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send(sentence, lang_str, style_str,\n",
    "         tokenizer, detokenizer, truecaser, segmenter,\n",
    "         translator):\n",
    "    lang_dict = {'EN': 'to-en',\n",
    "                 'ET': 'to-et',\n",
    "                 'LV': 'to-lv',\n",
    "                 'DE': 'to-de',\n",
    "                 'FR': 'to-fr'}\n",
    "    style_dict = {'Informal': 'to-osubs',\n",
    "                  'Official': 'to-eparl',\n",
    "                  'Legal': 'to-jrcac',\n",
    "                  'Medical': 'to-emea'}\n",
    "    return translate(sentence, lang_dict[lang_str], style_dict[style_str],\n",
    "                     tokenizer, detokenizer, truecaser, segmenter,\n",
    "                     translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN-ET-LV translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could not come to class today because I had a very important meeting, and also because my cat chewed my report.\n",
      "0.7731778621673584\n"
     ]
    }
   ],
   "source": [
    "# Choose target language\n",
    "TRG_LANG = 'EN'\n",
    "# TRG_LANG = 'ET'\n",
    "# TRG_LANG = 'LV'\n",
    "\n",
    "# Choose target style\n",
    "# TRG_STYLE = 'Informal'\n",
    "TRG_STYLE = 'Official'\n",
    "# TRG_STYLE = 'Legal'\n",
    "# TRG_STYLE = 'Medical'\n",
    "\n",
    "# Print sentence\n",
    "src_sent = 'I could not come to class today because I had a very important meeting, and also because my cat chewed my report.'\n",
    "\n",
    "start = time()\n",
    "print(send(src_sent, lang_str=TRG_LANG, style_str=TRG_STYLE,\n",
    "     tokenizer=my_tokenizer, detokenizer=my_detokenizer,\n",
    "     truecaser=my_truecaser_enetlv, segmenter=my_segmenter_enetlv, translator=my_translator_enetlv))\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "textw1 = widgets.Textarea(\n",
    "    value='Hello!',\n",
    "    placeholder='Source sentence',\n",
    "    description='',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='300px', height='90px')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "textw2 = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Translating...',\n",
    "    description='',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='300px', height='90px')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "langwidget = widgets.ToggleButtons(\n",
    "    options=['EN', 'ET', 'LV'],\n",
    "    description='Target lang:',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltips=['Translate into English', 'Translate into Estonian', 'Translate into Latvian'],\n",
    "    layout=widgets.Layout(width='600px', height='40px')   \n",
    ")\n",
    "\n",
    "langwidget.style.button_width='138px'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stylewidget = widgets.ToggleButtons(\n",
    "    options=['Informal', 'Official', 'Legal', 'Medical'],\n",
    "    description='Target style:',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltips=['Use colloquial style', 'Use official speech style', 'Use legal documents style', 'Use medical documents style'],\n",
    "    layout=widgets.Layout(width='600px', height='70px')\n",
    ")\n",
    "\n",
    "stylewidget.style.button_width='209px'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transl_button = widgets.Button(\n",
    "    description='Translate',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    tooltip='Translate sentence',\n",
    "    icon='',\n",
    "    layout=widgets.Layout(width='150px', height='35px')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_on_click(b):\n",
    "    textw2.value=''\n",
    "    tr = send(textw1.value, lang_str=langwidget.value, style_str=stylewidget.value,\n",
    "     tokenizer=my_tokenizer, detokenizer=my_detokenizer,\n",
    "     truecaser=my_truecaser_enetlv, segmenter=my_segmenter_enetlv, translator=my_translator_enetlv)\n",
    "    textw2.value=tr\n",
    "        \n",
    "transl_button.on_click(translate_on_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbox = widgets.HBox([textw1, textw2])\n",
    "buttonbox = widgets.HBox([transl_button])\n",
    "buttonbox.layout.padding = '0px 0px 0px 220px'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EN-ET-LV translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25870e057fae495caa757c3cee030498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Textarea(value='Hello!', layout=Layout(height='90px', width='300px'), placeholder='Source sente…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70b6ff7b5434cd6beff29574d4812a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Target lang:', layout=Layout(height='40px', width='600px'), options=('EN', 'ET', 'L…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ce8e38c878476592333e8701d05eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Target style:', layout=Layout(height='70px', width='600px'), options=('Informal', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd32717aca224a52a425bf3f342344c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Translate', layout=Layout(height='35px', width='150…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "textbox = widgets.HBox([textw1, textw2])\n",
    "buttonbox = widgets.HBox([transl_button])\n",
    "buttonbox.layout.padding = '0px 0px 0px 220px'\n",
    "display(textbox, langwidget, stylewidget, buttonbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
